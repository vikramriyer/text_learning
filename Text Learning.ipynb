{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.decomposition import PCA\n",
    "import re\n",
    "import HTMLParser\n",
    "import string\n",
    "import itertools\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "from gensim import utils\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import Doc2Vec\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'UserId', u'Score', u'Summary', u'Text', u'Rating'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Reviews.csv')\n",
    "df.drop('Id', inplace=True, axis=1)\n",
    "df.drop('ProductId', inplace=True, axis=1)\n",
    "df.drop('HelpfulnessNumerator', inplace=True, axis=1)\n",
    "df.drop('HelpfulnessDenominator', inplace=True, axis=1)\n",
    "df.drop('ProfileName', inplace=True, axis=1)\n",
    "df.drop('Time', inplace=True, axis=1)\n",
    "\n",
    "## Make this a binary classifier model dataset\n",
    "# 0 - bad, 1 - good\n",
    "df.drop(df[df.Score == 3].index, inplace=True)\n",
    "f = lambda x: 0 if x['Score'] < 3 else 1\n",
    "df['Rating'] = df.apply(f, axis=1)\n",
    "\n",
    "print df.columns\n",
    "\n",
    "df['Text'].fillna('', inplace=True)\n",
    "df['Summary'].fillna('', inplace=True)\n",
    "df['UserId'].fillna('', inplace=True)\n",
    "df['Score'].fillna(df['Score'].mean(), inplace=True)\n",
    "\n",
    "html_parser = HTMLParser.HTMLParser()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer('english')\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "def decode_characters_utf8(row):\n",
    "    return row.decode(\"utf8\").encode('ascii','ignore')\n",
    "\n",
    "def split_joint_words_and_convert_to_lower(row):\n",
    "    row = \" \".join(re.findall('[A-Z][^A-Z]*', row))\n",
    "    return row.lower()\n",
    "\n",
    "def remove_punctuations_at_last_or_first(row):\n",
    "    return ' '.join([word.strip(string.punctuation) for word in row.split(\" \")])\n",
    "\n",
    "def remove_html_tags(row):\n",
    "    return html_parser.unescape(row)\n",
    "\n",
    "def remove_urls(row):\n",
    "    return re.sub(r\"http\\S+\", \"\", row)\n",
    "\n",
    "def standardize_words(row):\n",
    "    return ''.join(''.join(s)[:2] for _, s in itertools.groupby(row))\n",
    "\n",
    "def remove_stop_words(row):\n",
    "    return ' '.join([w for w in word_tokenize(row) if not w in stop_words])\n",
    "\n",
    "def remove_spl_char(row):\n",
    "    return re.sub('[^a-zA-Z\\n\\.]', ' ', row)\n",
    "\n",
    "def stem_words(row):\n",
    "    return stemmer.stem(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pre_process_data(field):\n",
    "\n",
    "    pre_processed_data = []\n",
    "    \n",
    "    for row in df[field]:\n",
    "        row = decode_characters_utf8(row)\n",
    "        row = split_joint_words_and_convert_to_lower(row)\n",
    "        row = remove_punctuations_at_last_or_first(row)\n",
    "        row = remove_html_tags(row)\n",
    "        row = remove_urls(row)\n",
    "        row = remove_spl_char(row)\n",
    "        row = remove_stop_words(row)\n",
    "        row = stem_words(row)\n",
    "        \n",
    "        pre_processed_data.append(row)\n",
    "        \n",
    "    return pre_processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time on Summary: 72.7818119526 seconds\n"
     ]
    }
   ],
   "source": [
    "# Pre processing\n",
    "#t = time.time()\n",
    "#Text = pre_process_data('Text')\n",
    "#print \"Time on Text: {} seconds\".format(time.time() - t)\n",
    "t = time.time()\n",
    "Summary = pre_process_data('Summary')\n",
    "print \"Time on Summary: {} seconds\".format(time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dimensionality Reduction\n",
    "#pca = PCA()\n",
    "#pca = pca.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vectorization\n",
    "'''\n",
    "tfidf = (max_features = 10)\n",
    "X_text = tfidf.fit_transform(Summary).toarray()\n",
    "y_text = df.iloc[:,4].values\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_text , y_text, test_size = .2, random_state = 0)\n",
    "\n",
    "# train naive bayes classifier\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# predict results for test set\n",
    "predictor = clf.predict(X_test)\n",
    "\n",
    "# construct the confusion matrix\n",
    "cm = confusion_matrix(y_test, predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-1885ec233380>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'cm' is not defined"
     ]
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
